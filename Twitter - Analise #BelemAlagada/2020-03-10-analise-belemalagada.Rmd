---
title: "Análise da #BelemAlagada"
author: "Rafael Barbosa"
date: "`r format(Sys.time(), '%d de %B, %Y às %H:%M')`"
always_allow_html: true
output:
  html_document:
    keep_md: true
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

```


Estava eu olhando [meu twitter](https://twitter.com/RafaelbarbosaS_), enquanto caía um famoso toró que começou ainda de madrugada, quando vejo alguns tweets do [Belém Trânsito](https://twitter.com/belemtransito), um perfil que informa sobre a situação do trânsito em Belém, informando o caos nas ruas. E para ajudar a população divulgando os vídeos de alagamento, como [este](https://twitter.com/caboquisse/status/1236344836057444353?s=20), [este](https://twitter.com/belemtransito/status/1236429030829801473?s=20) ou [esta disputa de natação](https://twitter.com/belemtransito/status/1236429030829801473?s=20) em um dos canais que têm na cidade.


## Metodologia


Irei utilizar o software R (Rstudio) e pacote ```rtweet``` para buscar os tweets associados a "#BelemAlagada" e "chuva" nos últimos 10 dias. O período é por conta das condições da API do twitter que só nos permite fazer a busca neste intervalo de tempo. 



## Mão na massa

### Como baixar os dados 

Primeiramente para termos acesso a API do twitter para obter estes dados, devemos ter uma conta e criar um app no twitter (coisa dos desenvolvedores, nesse caso somos um destes).

O professor Kearney, da universidade de Missouri, ministrou um workshop em que explicou muito bem como criar um app no twitter e pegar suas keys o link está [aqui](https://mkearney.github.io/nicar_tworkshop/#1) (em inglês), assim como as funcionalidades do pacote ```rtweet``` que também podem ser acessadas [aqui](https://rtweet.info/). 


Como não as keys são pessoais e ligadas a sua conta, não posso mostrar a minha, correto ?


### Pacotes utilizados

Os pacotes utilizados para a presente análise foram:


1. rtweet
2. tidyverse
3. lubridate
4. tidytext
5. ggthemr
6. abjutils
7. DT
8. tm
9. wordcloud2
10. plotly

A forma de carregar estes pacotes (de algum jeito que vi na web mas não lembro em qual stackoverflow eu vi na vida, peço desculpas desde já) pode ser feita assim ó:


```{r pacotes, echo = TRUE, warning = FALSE, message = F}

if(!require(tidyverse)) {
  install.packages("tidyverse", dependencies = T);
  require(tidyverse)
}

if(!require(rtweet)) {
  install.packages("rtweet", dependencies = T);
  require(rtweet)
}


if(!require(lubridate)) {
  install.packages("lubridate", dependencies = T);
  require(lubridate)
}


if(!require(tidytext)) {
  install.packages("tidytext", dependencies = T);
  require(tidytext)
}


if(!require(ggthemr)) {
  install.packages("ggthemr", dependencies = T);
  require(ggthemr)
}


if(!require(abjutils)) {
  install.packages("abjutils", dependencies = T);
  require(abjutils)
}


if(!require(kableExtra)) {
  install.packages("kableExtra", dependencies = T);
  require(kableExtra)
}


if(!require(tm)) {
  install.packages("tm", dependencies = T);
  require(tm)
}


if(!require(wordcloud2)) {
  install.packages("wordcloud2", dependencies = T);
  require(wordcloud2)
}


if(!require(plotly)) {
  install.packages("plotly", dependencies = T);
  require(plotly)
}



```


```{r app_token, include = F} 

create_token(app = "Analise_onibus_belem",
             consumer_key = "u9uKZIBNJc5JZM1zXMebJZ2RD",
             consumer_secret = "UHp53D17HyDi2tR1Su10jSKtuW1QJZfVLFATm9L97zsBwNKkQe",
             access_token = "2415911374-o6z5sFv84VN0IiIFjuNkmHJmpEGK6w5D3oY8C2K",
             access_secret = "x380LQBxa0G8el4QK3EibIo2mmXSN8H009VOmTt43fFnJ")


```


### Configurações adicionais

A `formato_real_graf` vêm do [stackoverflow](https://pt.stackoverflow.com/questions/216852/adicionar-nota%C3%A7%C3%A3o-de-moeda-em-r) e `ggthemr` padroniza os gráficos em um tema específico chamado fresh.

```{r configs_adicionais} 

formato_real_graf <- function(values, nsmall = 0) { #- Formatando o valor como moeda
  values %>%
    as.numeric() %>%
    format(nsmall = nsmall, decimal.mark = ",", big.mark = ".") %>%
    str_trim()
}


ggthemr('fresh')


```



### Baixando os dados via API do twitter


Já com as keys e token criadas e setadas, podemos baixar os dados com a função ```search_tweets```.


```{r baixando_dados}


tweets <- search_tweets(q = "chuva OR #BelemAlagada", # Palavras-chave
                        include_rts = F, # Retirar os tweets que foram RT
                        n = 18000, # Número máximo de tweets
                        type = "recent", # Tweets mais recentes
                        geocode = "-1.404242,-48.435261,30km") # Em um raio de 40km do entroncamento


```


Temos um banco de dados com `r dim(tweets)[1]` linhas e `r dim(tweets)[2]` colunas, ou seja, `r dim(tweets)[1]` tweets com que continham a #BelemAlagada ou #chuva.



### Estrutura dos dados


A estrutura dos dados podem ser verificadas a seguir


```{r estrutura_dados}


tweets %>% 
  glimpse


```


### Modificação dos dados


Podemos verificar que temos uma variável que possui a localização dos tweets, vamos mostrar algumas randômicas.


```{r mostrar_local_dados, echo = F}

set.seed(1234)

tweets %>% 
  select(location) %>% 
  sample_n(size = 10) %>%
  kable %>% 
  kable_styling(bootstrap_options = "striped", full_width = F)
```

Podemos perceber que temos regiões como Ananindeua, Belém e Castanhal, claro que estão a um raio de 30km do entroncamento, devido a isto aparecem. Mas podemos verificar que Belém possui variações de escrita (belém, Nárnia - Belém, etc). Logo devemos unificar elas com algumas modificações.


```{r modific_dados, echo = F}

banco <-
  tweets %>%
  mutate(location = rm_accent(x = location),
         location = str_to_lower(string = location)) %>%
  filter(str_detect(string = location, pattern = "belem") |
         str_detect(string = location, pattern = "ananindeua") |
           str_detect(string = location, pattern = "castanhal") |
           str_detect(string = location, pattern = "marituba") |
           str_detect(string = location, pattern = "benevides")) %>%
  mutate(nova_local =
           case_when(
             grepl("belem", location) ~ "Belém",
             grepl("ananindeua", location) ~ "Ananindeua",
             grepl("castanhal", location) ~ "Castanhal",
             grepl("marituba", location) ~ "Marituba",
             grepl("benevides", location) ~ "Benevides",
           ))


banco %>%
  select(location, nova_local) %>%
  sample_n(size = 10) %>% 
  kable %>% 
  kable_styling(bootstrap_options = "striped", full_width = F)

```


### Localização dos tweets 


Agora que modificamos a variável `nova_local` podemos fazer quais são as localizações mais frequentes nos tweets.


```{r localizacao_tweets, echo = F, fig.align = "center", fig.width = 10, fig.height = 6}

localizacao_tweets <- 
  banco %>%
  count(nova_local) %>%
  mutate(perc = round(x = n/sum(n)*100, digits = 2),
         br_perc = formato_real_graf(values = perc, nsmall = 2)) %>%
  ggplot(data = .) +
  geom_bar(aes(x = reorder(nova_local, perc), y = perc), stat = "identity", colour = "black",
           fill = "darkblue", width = 0.5, alpha = 0.4) +
  geom_text(aes(x = reorder(nova_local, perc), y = perc,
                label = paste0(br_perc, "%")),
            vjust = -0.7, size = 5) +
  scale_y_continuous(labels = function(x) paste0(x, "%"),
                     limits = c(0, 100)) +
  theme(legend.position = "bottom",
        axis.title.y = element_text(colour = "black", face = "bold", size = 14),
        axis.title.x = element_text(colour = "black", face = "bold", size = 14),
        axis.text = element_text(colour = "black", size = 14),
        strip.text.x = element_text(size = 12, colour = "white"),
        strip.text.y = element_text(size = 12, colour = "white"),
        legend.title = element_text(size = 16, color = "black", face = "bold"),
        legend.text = element_text(size = 16, color = "black"),
        axis.line = element_line(size = 0.5, colour = "black"),
        plot.caption = element_text(size = 12, color = "black",
                                    vjust = 7),
        plot.subtitle = element_text(size = 12, color = "black"),
        plot.title = element_text(size = 18, face = "bold",
                                  hjust = 0.5, color = "black")) +
  labs(title = "Localização dos usuários dos tweets", 
       x = "Município", y = "", 
       subtitle = paste0("Tweets analisados = ", 
                         formato_real_graf(length(banco$user_id))))

localizacao_tweets

  
```


Podemos perceber que a maioria (esmagadora) dos tweets relacionadas a #BelemAlagada e a chuva era realmente em Belém, mas não podemos tirar o fato que ocorreu nos outros municípios próximos de Belém. 


### Quantos tweets relacionados por dia (thinking) ?

```{r quantidade_grafico, echo = F, fig.align = "center", fig.width = 10, fig.height = 6}

quantidade_grafico <- 
  tweets %>% 
  ts_plot(by = "3 hours", tz = "America/Belem") +
  theme(legend.position = "bottom",
        axis.title.y = element_text(colour = "black", face = "bold", size = 14),
        axis.title.x = element_text(colour = "black", face = "bold", size = 14),
        axis.text = element_text(colour = "black", size = 14),
        strip.text.x = element_text(size = 12, colour = "white"),
        strip.text.y = element_text(size = 12, colour = "white"),
        legend.title = element_text(size = 16, color = "black", face = "bold"),
        legend.text = element_text(size = 16, color = "black"),
        axis.line = element_line(size = 0.5, colour = "black"),
        plot.caption = element_text(size = 12, color = "black",
                                    vjust = 7),
        plot.subtitle = element_text(size = 12, color = "black"),
        plot.title = element_text(size = 18, face = "bold",
                                  hjust = 0.5, color = "black")) +
  labs(x = "", y = "",
       title = "Quantidade de tweets por dia")


quantidade_grafico


```


Podemos verificar há um pico de mais de 2000 tweets relacionados a #BelemAlagada ou chuva na região de Belém, durante este período de chuvas. 


### Quais os usuários que mais mencionaram #BelemAlagada ou chuva


```{r top_n_usuarios, warning = F, message = F, echo = F}

banco %>% 
  count(screen_name, sort = T) %>% 
  magrittr::set_colnames(c("Usuário", "Frequência")) %>% 
  head(10) %>% 
  kable %>% 
  kable_styling(bootstrap_options = "striped", full_width = F)

```
Temos que mencionar o grande trabalho do [Belém Trânsito](https://twitter.com/belemtransito) que compartilha grandes informações com todo o twitter sobre a situação de Belém, não somente neste período de chuvas, mas em literalmente qualquer problema, vai de chuva a greve de ônibus. Inclusive, se você comentou algo sobre chuva o #BelemAlagada, veja se seu @ não está aí com a quantidade ;)


### Quais as palavras mais utilizadas com a #BelemAlagada ou chuva ?


Inicialmente temos que passar por um processo de limpeza dos tweets e remover pontuações, espaços em brancos, etc, para que fique somente as palavras corretas para que podemos associar a um dicionário. A função utilizada foi a `cleanTweets` do [Fellipe Gomes](https://gomesfellipe.github.io/) que pode ser encontrada [aqui](https://github.com/gomesfellipe/functions/blob/master/cleanTweets.R)  com algumas modificações.

Posteriormente utilizar a função `unnest_tokens` do pacote _tidytext_ para que ele separe cada palavra por linha, fazendo com o que banco aumente, apesar de tudo. 


```{r clean_tweets, include = F}

cleanTweets <- function(tweet){

  # Limpe o tweet para an?lise de sentimentos

  tweet = gsub("(f|ht)(tp)(s?)(://)(.*)[.|/](.*)", " ", tweet)  # Remove html links
  tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", " ", tweet)       # Remove retweet
  tweet = gsub("#\\w+", " ", tweet)                             # Remove todos "#Hashtag"
  tweet = gsub("@\\w+", " ", tweet)                             # Remove todos "@people"
  tweet = gsub("[[:punct:]]", " ", tweet)                       # Remove todas as pontuacoes
  tweet = gsub("[[:digit:]]", " ", tweet)                       # Remover numeros, precisamos apenas de texto para an?lise

  tweet = gsub("[ \t]{2,}", " ", tweet)                         # Remove espa?os desnecessarios
  tweet = gsub("^\\s+|\\s+$", "", tweet)                        # (espacos em branco, tabs etc)

  tweet = gsub('https://','',tweet)                             # Remove https://
  tweet = gsub('http://','',tweet)                              # Remove http://
  tweet = gsub('[^[:graph:]]', ' ',tweet)                       # Remove strings gr?ficos como emoticons
  tweet = gsub('[[:punct:]]', '', tweet)                        # Remove pontuacao
  tweet = gsub('[[:cntrl:]]', '', tweet)                        # Remove strings de controle
  tweet = gsub('\\d+', '', tweet)                               # Remove numeros
  tweet=str_replace_all(tweet,"[^[:graph:]]", " ")              # Remove strings gr?ficos como emoticons
  #tweet=SnowballC::wordStem(tweet,language = lang)     # Aplica steamming (desativado)

  #Converte tudo para minusculo

  return(tweet)
}

```



```{r, banco_token, echo = F}

banco_token <- 
  tweets %>%
  mutate(text = cleanTweets(tweet = text)) %>%
  unnest_tokens(word, text)



banco_token %>%
  select(word) %>% 
  head(10) %>% 
  kable %>% 
  kable_styling(bootstrap_options = "striped", full_width = F)
  

```

Agora iremos buscar o dicionário (ou stop words) do pacote `tm`, fazer um [anti_join](https://dplyr.tidyverse.org/reference/join.html), retorna todas as linhas de x onde não há valores correspondentes em y, mantendo apenas as colunas de x e filtras pelas palavras que não é "Belém", já que sabemos que ela faz parte da pesquisa inicial.


```{r palavras, include = F}

banco_palavras <- data.frame(word = c(tm::stopwords("pt"), "pra"))

banco_palavras %>% 
  head(10) %>% 
  kable %>% 
  kable_styling(bootstrap_options = "striped", full_width = F)


```

```{r mostrar_banco_freq, echo = F, warning = F, message = F}

banco_freq <- 
  banco_token %>%
  anti_join(banco_palavras, by = "word") %>%
  count(word, sort = T) %>%
  filter(word != "chuva")

banco_freq %>% 
  head(10) %>% 
  kable %>% 
  kable_styling(bootstrap_options = "striped", full_width = F)

```

E agora podemos ver em uma nuvem de palavras, que talvez seja a visualização de dados mais adequada, com o pacote `wordcloud2`.

```{r plotapeloamordegod}


wordcloud2(data = banco_freq, size = .7,
           shape = "oval",
           rotateRatio = 0.5,
           ellipticity = 0.9, color = "random-dark")


```
